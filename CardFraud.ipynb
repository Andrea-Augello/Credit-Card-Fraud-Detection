{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CardFraud.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NRaXG6yTr5Nn",
        "nBZK-RwBNxVC"
      ],
      "authorship_tag": "ABX9TyPcUW7WWdM1XMIeScjKWns8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoK-VO7eYJFb"
      },
      "source": [
        "# Credit Card Fraud Detection\n",
        "\n",
        "Il dataset contiene transazioni fatte tramite carta di credito nel settembre 2013 in Europa.\n",
        "Questo dataset presenta transazioni avvenute in due giorni. Delle 284807 transazioni (di cui 1081 duplicate), 492 sono etichettate come frodi, le altre sono da ritenersi legittime. Il dataset è estremamente sbilanciato, con i casi positivi che rappresentano soltanto lo 0.172% di tutte le transazioni.\n",
        "\n",
        "Nel dataset sono presenti soltanto variabili di input di tipo numerico, risultanti da una trasformazione Principal Component Analysis (PCA). Per ragioni di confidenzialità non sono disponibili né le features originali né informazioni sull'origine dei dati più dettagliate. Le uniche features non trasformate tramite PCA sono 'Tempo' e 'Ammontare'.\n",
        "Il tempo misura i sencondi trascorsi tra ogni transazione e la prima transazione del dataset. La feature 'Ammontare' è l'ammontare della transazione.\n",
        "Le transazioni etichettate con 1 sono frodi, quelle etichettate con 0 sono legittime.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_OYkQOn8lhg"
      },
      "source": [
        "## Setup dell'ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l3QYwwObNPd",
        "outputId": "1d830e70-a7d5-41f3-c4c1-d8baab501329"
      },
      "source": [
        "! rm -Rf /content/creditfraud/\n",
        "! which kaggle || pip install kaggle\n",
        "! if ! [ -d ~/.kaggle ] ; then  mkdir ~/.kaggle  && touch ~/.kaggle/kaggle.json; echo \"Creating kaggle directory\"; fi\n",
        "\n",
        "import json\n",
        "\n",
        "token = {\"username\":\"andreaaugello\",\"key\":\"636f00800308e8447dea0ee8f2decb98\"}\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "\n",
        "! if ! [ -f /content/creditfraud/creditcard.csv ] ; then chmod 600 /root/.kaggle/kaggle.json && kaggle datasets download -p /content/creditfraud/ -d mlg-ulb/creditcardfraud ; fi\n",
        "\n",
        "os.chdir('/content/creditfraud')\n",
        "for file in os.listdir():\n",
        "    zip_ref = zipfile.ZipFile(file, 'r')\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/kaggle\n",
            "Creating kaggle directory\n",
            "Downloading creditcardfraud.zip to /content/creditfraud\n",
            " 89% 59.0M/66.0M [00:00<00:00, 60.1MB/s]\n",
            "100% 66.0M/66.0M [00:00<00:00, 96.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnY3ywNtk3Zh",
        "outputId": "3fd58901-62b4-4c46-e8f8-06a14f18ee43"
      },
      "source": [
        "!head -n 2 /content/creditfraud/creditcard.csv\n",
        "!tail -n 2 /content/creditfraud/creditcard.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
            "0,-1.3598071336738,-0.0727811733098497,2.53634673796914,1.37815522427443,-0.338320769942518,0.462387777762292,0.239598554061257,0.0986979012610507,0.363786969611213,0.0907941719789316,-0.551599533260813,-0.617800855762348,-0.991389847235408,-0.311169353699879,1.46817697209427,-0.470400525259478,0.207971241929242,0.0257905801985591,0.403992960255733,0.251412098239705,-0.018306777944153,0.277837575558899,-0.110473910188767,0.0669280749146731,0.128539358273528,-0.189114843888824,0.133558376740387,-0.0210530534538215,149.62,\"0\"\n",
            "172788,-0.240440049680947,0.530482513118839,0.702510230095103,0.689799168040973,-0.377961134444982,0.62370772214768,-0.68617998628885,0.679145459790659,0.392086712465972,-0.399125651432835,-1.93384881505713,-0.962886142890271,-1.04208165591191,0.449624443166001,1.96256312066577,-0.60857712704613,0.509928460110321,1.11398059049908,2.89784877334313,0.127433515805355,0.265244916386865,0.800048741498139,-0.163297944406659,0.123205243742508,-0.569158864158597,0.546668462188323,0.108820734744839,0.104532821478796,10,\"0\"\n",
            "172792,-0.53341252200504,-0.189733337002305,0.703337366963779,-0.506271240328258,-0.0125456787599659,-0.649616685713792,1.57700625437629,-0.414650407552662,0.486179505267237,-0.915426648905893,-1.04045833522361,-0.0315130540252157,-0.188092900791737,-0.0843164698151014,0.0413334553360658,-0.302620086427415,-0.660376645182784,0.16742993371973,-0.256116871098099,0.382948104875066,0.261057330790975,0.643078437820093,0.376777014169917,0.00879737940024202,-0.473648703898825,-0.818267121041176,-0.00241530880001015,0.0136489143320671,217,\"0\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVtOWVHDQ88n",
        "outputId": "4684b47a-e9cc-4388-eac5-9b973cc49a93"
      },
      "source": [
        "%%shell\n",
        "\n",
        "apt-get install openjdk-8-jdk-headless -qq &>/dev/null\n",
        "if ! [ -f spark-3.1.1-bin-hadoop2.7.tgz ] ; then \n",
        "  wget -q http://apache.osuosl.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz && \n",
        "  tar xf spark-3.1.1-bin-hadoop2.7.tgz ; \n",
        "fi\n",
        "pip show findspark &>/dev/null || pip install -q findspark\n",
        "pip show pyspark &>/dev/null || pip install pyspark\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 74kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 48.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=7846ec8f40543f2ccad174b1cde67ffcae89609e81dfe26d07d533a120f6f46a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vg42DtScdf9"
      },
      "source": [
        "# Set up required environment variables\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/creditfraud/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "! mkdir -p $SPARK_HOME/conf/\n",
        "! echo \"spark.driver.memory              8g\" >> $SPARK_HOME/conf/spark-defaults.conf\n",
        "\n",
        "import findspark\n",
        "location = findspark.find()\n",
        "findspark.init(location)\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create the session\n",
        "\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "conf.set(\"spark.driver.memory\", \"8g\") \n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "\n",
        "spark = SparkSession.builder.appName('CardFraud').getOrCreate()\n",
        "SparkContext.setSystemProperty('spark.executor.memory', '8g')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1m2-XqdQPlt",
        "outputId": "ebe8cd7f-2234-4bce-f5e5-8954a4986460"
      },
      "source": [
        "# Allow checking Spark UI through ngrok tunnel\n",
        "\n",
        "! [ -f ngrok-stable-linux-amd64.zip ] || wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! [ -f ngrok ] || unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://1b42f51cd92c.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiypUJeBSIID"
      },
      "source": [
        "## Analisi dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "HoS8URo5kvcC",
        "outputId": "0e908176-5d9a-45a9-cafe-245b0a95f2a9"
      },
      "source": [
        "# Importazione dei dati dal data set \n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "# costruzione dello schema del DataFrame\n",
        "\n",
        "creditcard_schema = StructType([\n",
        "    StructField('Time',IntegerType(),False),\n",
        "    StructField('V1',FloatType(),False),\n",
        "    StructField('V2',FloatType(),False),\n",
        "    StructField('V3',FloatType(),False),\n",
        "    StructField('V4',FloatType(),False),\n",
        "    StructField('V5',FloatType(),False),\n",
        "    StructField('V6',FloatType(),False),\n",
        "    StructField('V7',FloatType(),False),\n",
        "    StructField('V8',FloatType(),False),\n",
        "    StructField('V9',FloatType(),False),\n",
        "    StructField('V10',FloatType(),False),\n",
        "    StructField('V11',FloatType(),False),\n",
        "    StructField('V12',FloatType(),False),\n",
        "    StructField('V13',FloatType(),False),\n",
        "    StructField('V14',FloatType(),False),\n",
        "    StructField('V15',FloatType(),False),\n",
        "    StructField('V16',FloatType(),False),\n",
        "    StructField('V17',FloatType(),False),\n",
        "    StructField('V18',FloatType(),False),\n",
        "    StructField('V19',FloatType(),False),\n",
        "    StructField('V20',FloatType(),False),\n",
        "    StructField('V21',FloatType(),False),\n",
        "    StructField('V22',FloatType(),False),\n",
        "    StructField('V23',FloatType(),False),\n",
        "    StructField('V24',FloatType(),False),\n",
        "    StructField('V25',FloatType(),False),\n",
        "    StructField('V26',FloatType(),False),\n",
        "    StructField('V27',FloatType(),False),\n",
        "    StructField('V28',FloatType(),False),\n",
        "    StructField('Amount',FloatType(),False),\n",
        "    StructField('Class',ShortType(),False),\n",
        "])\n",
        "\n",
        "records = spark.read.format('csv')\\\n",
        "    .option('header','true')\\\n",
        "    .option('mode','DROPMALFORMED')\\\n",
        "    .option('mode','PERMISSIVE')\\\n",
        "    .schema(creditcard_schema)\\\n",
        "    .load('/content/creditfraud/creditcard.csv')\n",
        "\n",
        "#records.printSchema()\n",
        "#records.show()\n",
        "\n",
        "display(records)\n",
        "display(records.describe().toPandas())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[Time: int, V1: float, V2: float, V3: float, V4: float, V5: float, V6: float, V7: float, V8: float, V9: float, V10: float, V11: float, V12: float, V13: float, V14: float, V15: float, V16: float, V17: float, V18: float, V19: float, V20: float, V21: float, V22: float, V23: float, V24: float, V25: float, V26: float, V27: float, V28: float, Amount: float, Class: smallint]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>count</td>\n",
              "      <td>284806</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "      <td>284807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mean</td>\n",
              "      <td>94813.84136570156</td>\n",
              "      <td>-2.237831565309384E-10</td>\n",
              "      <td>5.6390376770574364E-11</td>\n",
              "      <td>5.7181949824170215E-11</td>\n",
              "      <td>-5.515661110814551E-11</td>\n",
              "      <td>2.6428014635959684E-11</td>\n",
              "      <td>-1.9836218453151118E-10</td>\n",
              "      <td>-1.1044655188441163E-10</td>\n",
              "      <td>5.3475000568427694E-11</td>\n",
              "      <td>2.419927089644845E-11</td>\n",
              "      <td>5.3895441523821255E-11</td>\n",
              "      <td>-6.56758850695699E-11</td>\n",
              "      <td>4.793081498147977E-11</td>\n",
              "      <td>2.1484545434430527E-11</td>\n",
              "      <td>-8.998896774441982E-11</td>\n",
              "      <td>-4.6294032998024574E-11</td>\n",
              "      <td>-5.987137419976068E-11</td>\n",
              "      <td>3.639741548440692E-11</td>\n",
              "      <td>1.5957350183256596E-11</td>\n",
              "      <td>2.9739322663330695E-11</td>\n",
              "      <td>3.740233772192539E-11</td>\n",
              "      <td>-1.1423847676931417E-11</td>\n",
              "      <td>4.026301784830157E-11</td>\n",
              "      <td>-9.643035035753393E-12</td>\n",
              "      <td>4.7671430945018126E-11</td>\n",
              "      <td>-7.220635637499166E-12</td>\n",
              "      <td>-4.4536415414434063E-11</td>\n",
              "      <td>2.3264761314214363E-11</td>\n",
              "      <td>-1.0009771058213764E-11</td>\n",
              "      <td>88.34961924204623</td>\n",
              "      <td>0.001727485630620034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stddev</td>\n",
              "      <td>47488.22832975011</td>\n",
              "      <td>1.958695804149988</td>\n",
              "      <td>1.6513085797790983</td>\n",
              "      <td>1.5162550047888008</td>\n",
              "      <td>1.4158685748057576</td>\n",
              "      <td>1.38024673467619</td>\n",
              "      <td>1.3322710895129717</td>\n",
              "      <td>1.2370935982054971</td>\n",
              "      <td>1.1943529028329143</td>\n",
              "      <td>1.0986320894527717</td>\n",
              "      <td>1.0888497654131535</td>\n",
              "      <td>1.0207130274845488</td>\n",
              "      <td>0.9992013897133897</td>\n",
              "      <td>0.9952742301172138</td>\n",
              "      <td>0.9585956107110349</td>\n",
              "      <td>0.9153160115854574</td>\n",
              "      <td>0.8762528874680678</td>\n",
              "      <td>0.8493370639017923</td>\n",
              "      <td>0.8381762093603176</td>\n",
              "      <td>0.8140405007230054</td>\n",
              "      <td>0.7709250247479664</td>\n",
              "      <td>0.7345240139851617</td>\n",
              "      <td>0.7257015605697456</td>\n",
              "      <td>0.6244602954715794</td>\n",
              "      <td>0.6056470678589732</td>\n",
              "      <td>0.52127807053526</td>\n",
              "      <td>0.48222701319019107</td>\n",
              "      <td>0.4036324945392819</td>\n",
              "      <td>0.33008326434426066</td>\n",
              "      <td>250.12010901734928</td>\n",
              "      <td>0.04152718963546528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>min</td>\n",
              "      <td>0</td>\n",
              "      <td>-56.40751</td>\n",
              "      <td>-72.71573</td>\n",
              "      <td>-48.32559</td>\n",
              "      <td>-5.6831713</td>\n",
              "      <td>-113.74331</td>\n",
              "      <td>-26.160505</td>\n",
              "      <td>-43.557243</td>\n",
              "      <td>-73.21672</td>\n",
              "      <td>-13.434067</td>\n",
              "      <td>-24.588263</td>\n",
              "      <td>-4.7974734</td>\n",
              "      <td>-18.683714</td>\n",
              "      <td>-5.791881</td>\n",
              "      <td>-19.214325</td>\n",
              "      <td>-4.4989448</td>\n",
              "      <td>-14.129854</td>\n",
              "      <td>-25.1628</td>\n",
              "      <td>-9.498746</td>\n",
              "      <td>-7.213527</td>\n",
              "      <td>-54.49772</td>\n",
              "      <td>-34.830383</td>\n",
              "      <td>-10.933144</td>\n",
              "      <td>-44.807735</td>\n",
              "      <td>-2.836627</td>\n",
              "      <td>-10.295397</td>\n",
              "      <td>-2.6045506</td>\n",
              "      <td>-22.56568</td>\n",
              "      <td>-15.430084</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>max</td>\n",
              "      <td>172792</td>\n",
              "      <td>2.45493</td>\n",
              "      <td>22.05773</td>\n",
              "      <td>9.382559</td>\n",
              "      <td>16.875343</td>\n",
              "      <td>34.801666</td>\n",
              "      <td>73.30163</td>\n",
              "      <td>120.58949</td>\n",
              "      <td>20.007208</td>\n",
              "      <td>15.594995</td>\n",
              "      <td>23.745136</td>\n",
              "      <td>12.018913</td>\n",
              "      <td>7.848392</td>\n",
              "      <td>7.126883</td>\n",
              "      <td>10.526766</td>\n",
              "      <td>8.877742</td>\n",
              "      <td>17.315111</td>\n",
              "      <td>9.253527</td>\n",
              "      <td>5.041069</td>\n",
              "      <td>5.5919714</td>\n",
              "      <td>39.420906</td>\n",
              "      <td>27.202839</td>\n",
              "      <td>10.50309</td>\n",
              "      <td>22.528412</td>\n",
              "      <td>4.584549</td>\n",
              "      <td>7.5195885</td>\n",
              "      <td>3.5173457</td>\n",
              "      <td>31.612198</td>\n",
              "      <td>33.84781</td>\n",
              "      <td>25691.16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  summary               Time  ...              Amount                 Class\n",
              "0   count             284806  ...              284807                284807\n",
              "1    mean  94813.84136570156  ...   88.34961924204623  0.001727485630620034\n",
              "2  stddev  47488.22832975011  ...  250.12010901734928   0.04152718963546528\n",
              "3     min                  0  ...                 0.0                     0\n",
              "4     max             172792  ...            25691.16                     1\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBCtAwhcw1oh"
      },
      "source": [
        "Possiamo vedere che in uno dei record manca il campo del tempo, cerchiamo quindi di individuarlo ed, eventualmente, correggere l'errore.\n",
        "\n",
        "Cerchiamo nel file CSV il record che non inizia con un numero intero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woX8qrQJz9X8",
        "outputId": "6e9754af-8163-4a22-9140-a77609ebc8f0"
      },
      "source": [
        "! grep -v \"^[0-9]*,\" creditcard.csv "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
            "1e+05,-1.40863322829137,-1.624697936324,2.54774230369692,0.385671523516146,0.502790072699087,0.507194721385658,-1.74443114473473,0.760594225747498,3.00170400322912,-1.16309493493591,0.393516209370638,-1.9761080196344,0.231283936059375,0.526783847087809,-3.23246747614055,0.021830807795285,0.519902436567202,1.12642627868296,0.0800098779433671,0.209032736561698,0.21764082393781,0.758246770655715,0.281254134069628,0.736607975853892,-0.741402076374565,0.255349902866452,0.141944167181525,0.228167205092217,49.5,\"0\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6WdCQlf0Rmd"
      },
      "source": [
        "Risulta essere un record in cui il tempo è scritto in notazione scientifica come $1\\cdot 10^{5}$.\n",
        "Appartiene alla classe delle transazioni legittime già sovrarappresentata, quindi non è molto importante mantenerlo, ma lo correggiamo ugualmente sostituendo il valore `null` con $100000$. \n",
        "\n",
        "Essendoci soltanto una riga con tempo nullo possiamo utilizzare la trasformazione `withColumn` senza problemi. Adesso che non abbiamo più valori nulli rimuoviamo le righe duplicate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "ZOEuyan4xXrk",
        "outputId": "9e555180-0a96-4802-df3e-48b371f894e5"
      },
      "source": [
        "records = records.withColumn(\"Time\",\n",
        "                             when(records[\"Time\"].isNull(),\n",
        "                                  100000) \\\n",
        "                             .otherwise(records[\"Time\"])).distinct()\n",
        "\n",
        "display(records.describe().toPandas())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>count</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "      <td>283726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mean</td>\n",
              "      <td>94811.07759951502</td>\n",
              "      <td>0.0059171496151878295</td>\n",
              "      <td>-0.004134755596655358</td>\n",
              "      <td>0.0016131194013566076</td>\n",
              "      <td>-0.0029663077763300767</td>\n",
              "      <td>0.0018275601352146962</td>\n",
              "      <td>-0.0011394883794042796</td>\n",
              "      <td>0.0018006916556496844</td>\n",
              "      <td>-8.544525275343677E-4</td>\n",
              "      <td>-0.0015961996029800678</td>\n",
              "      <td>-0.0014407104420079613</td>\n",
              "      <td>2.0175758274159E-4</td>\n",
              "      <td>-7.147876010724789E-4</td>\n",
              "      <td>6.033758161790353E-4</td>\n",
              "      <td>2.523172098282817E-4</td>\n",
              "      <td>0.0010428379946567953</td>\n",
              "      <td>0.0011620127510330307</td>\n",
              "      <td>1.7016099417563552E-4</td>\n",
              "      <td>0.0015151660271279712</td>\n",
              "      <td>-2.6426355734881E-4</td>\n",
              "      <td>1.8717529004213355E-4</td>\n",
              "      <td>-3.705931333337562E-4</td>\n",
              "      <td>-1.5027521572740273E-5</td>\n",
              "      <td>1.9817071581241128E-4</td>\n",
              "      <td>2.1420687436297502E-4</td>\n",
              "      <td>-2.3238699909439164E-4</td>\n",
              "      <td>1.4944104385428625E-4</td>\n",
              "      <td>0.001763031635814916</td>\n",
              "      <td>5.473120965431608E-4</td>\n",
              "      <td>88.47268730080253</td>\n",
              "      <td>0.001667101358352777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>stddev</td>\n",
              "      <td>47481.0478906195</td>\n",
              "      <td>1.9480261419053075</td>\n",
              "      <td>1.6467029644100455</td>\n",
              "      <td>1.508681915996848</td>\n",
              "      <td>1.4141840143251612</td>\n",
              "      <td>1.3770082799610668</td>\n",
              "      <td>1.331930591492362</td>\n",
              "      <td>1.227663895509697</td>\n",
              "      <td>1.1790544275868806</td>\n",
              "      <td>1.0954924812185733</td>\n",
              "      <td>1.0764073502841949</td>\n",
              "      <td>1.0187201524646452</td>\n",
              "      <td>0.9946744452641613</td>\n",
              "      <td>0.9954296367531891</td>\n",
              "      <td>0.9522150895082739</td>\n",
              "      <td>0.914893633435804</td>\n",
              "      <td>0.8736963276905809</td>\n",
              "      <td>0.8425073208639574</td>\n",
              "      <td>0.8373775296067039</td>\n",
              "      <td>0.8133785530591242</td>\n",
              "      <td>0.7699842412764643</td>\n",
              "      <td>0.723909366672215</td>\n",
              "      <td>0.7245504655227913</td>\n",
              "      <td>0.6237023784485339</td>\n",
              "      <td>0.605626698190032</td>\n",
              "      <td>0.5212203166769497</td>\n",
              "      <td>0.48205294076037</td>\n",
              "      <td>0.3957438805701256</td>\n",
              "      <td>0.3280266044291142</td>\n",
              "      <td>250.3994368876579</td>\n",
              "      <td>0.04079617625933865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>min</td>\n",
              "      <td>0</td>\n",
              "      <td>-56.40751</td>\n",
              "      <td>-72.71573</td>\n",
              "      <td>-48.32559</td>\n",
              "      <td>-5.6831713</td>\n",
              "      <td>-113.74331</td>\n",
              "      <td>-26.160505</td>\n",
              "      <td>-43.557243</td>\n",
              "      <td>-73.21672</td>\n",
              "      <td>-13.434067</td>\n",
              "      <td>-24.588263</td>\n",
              "      <td>-4.7974734</td>\n",
              "      <td>-18.683714</td>\n",
              "      <td>-5.791881</td>\n",
              "      <td>-19.214325</td>\n",
              "      <td>-4.4989448</td>\n",
              "      <td>-14.129854</td>\n",
              "      <td>-25.1628</td>\n",
              "      <td>-9.498746</td>\n",
              "      <td>-7.213527</td>\n",
              "      <td>-54.49772</td>\n",
              "      <td>-34.830383</td>\n",
              "      <td>-10.933144</td>\n",
              "      <td>-44.807735</td>\n",
              "      <td>-2.836627</td>\n",
              "      <td>-10.295397</td>\n",
              "      <td>-2.6045506</td>\n",
              "      <td>-22.56568</td>\n",
              "      <td>-15.430084</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>max</td>\n",
              "      <td>172792</td>\n",
              "      <td>2.45493</td>\n",
              "      <td>22.05773</td>\n",
              "      <td>9.382559</td>\n",
              "      <td>16.875343</td>\n",
              "      <td>34.801666</td>\n",
              "      <td>73.30163</td>\n",
              "      <td>120.58949</td>\n",
              "      <td>20.007208</td>\n",
              "      <td>15.594995</td>\n",
              "      <td>23.745136</td>\n",
              "      <td>12.018913</td>\n",
              "      <td>7.848392</td>\n",
              "      <td>7.126883</td>\n",
              "      <td>10.526766</td>\n",
              "      <td>8.877742</td>\n",
              "      <td>17.315111</td>\n",
              "      <td>9.253527</td>\n",
              "      <td>5.041069</td>\n",
              "      <td>5.5919714</td>\n",
              "      <td>39.420906</td>\n",
              "      <td>27.202839</td>\n",
              "      <td>10.50309</td>\n",
              "      <td>22.528412</td>\n",
              "      <td>4.584549</td>\n",
              "      <td>7.5195885</td>\n",
              "      <td>3.5173457</td>\n",
              "      <td>31.612198</td>\n",
              "      <td>33.84781</td>\n",
              "      <td>25691.16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  summary               Time  ...             Amount                 Class\n",
              "0   count             283726  ...             283726                283726\n",
              "1    mean  94811.07759951502  ...  88.47268730080253  0.001667101358352777\n",
              "2  stddev   47481.0478906195  ...  250.3994368876579   0.04079617625933865\n",
              "3     min                  0  ...                0.0                     0\n",
              "4     max             172792  ...           25691.16                     1\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10-ntEiPSwYx"
      },
      "source": [
        "Creiamo una funzione per separare il dataset in base alla classe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMIdeb1eVE04"
      },
      "source": [
        "def split_categories(dataset, describe=False):\n",
        "  fraud = dataset.filter(dataset.Class.contains(1))\n",
        "  legit = dataset.filter(dataset.Class.contains(0))\n",
        "  \n",
        "  if describe :\n",
        "\n",
        "    print(\"Fraudolent transactions: \",fraud.count())\n",
        "    print(\"Legit transactions:      \",legit.count())\n",
        "    print(\"-\"*40)\n",
        "    print(\"Total transactions:      \",dataset.count())\n",
        "  \n",
        "    print(\"Fraudolent transactions summary:\")\n",
        "    display(fraud.describe().toPandas())\n",
        "    print(\"Legit transactions summary:\")\n",
        "    display(legit.describe().toPandas())\n",
        "    \n",
        "  return (fraud, legit)\n",
        "\n",
        "\n",
        "fraud, legit = split_categories(records, describe=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnCfNU1ytA9"
      },
      "source": [
        "I descrittori statistici ottenuti precedentemente sono abbastanza differenti per le due classi, osserviamo adesso la forma delle distribuzioni delle varie feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La-ulRmh4wgh"
      },
      "source": [
        "# Imports needed for figures and plots\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSBQAEBjr-Oo"
      },
      "source": [
        "for col in fraud.columns[0:-1]:\n",
        "  hist_l = legit.select(col).rdd.flatMap(lambda x: x).histogram(30)\n",
        "  hist_f = fraud.select(col).rdd.flatMap(lambda x: x).histogram(30)\n",
        "\n",
        "  # Loading the Computed Histogram into a Pandas Dataframe for plotting\n",
        "  fig = plt.figure()\n",
        "  plt.title(col)\n",
        "  plt.axis(False)\n",
        "  ax_l = fig.add_subplot(1,2,1)\n",
        "  ax_f = fig.add_subplot(1,2,2)\n",
        "  pd.DataFrame( list(zip(*hist_l)),columns=['bin', 'frequency'] ) \\\n",
        "    .set_index('bin') \\\n",
        "    .plot(kind='bar', xlabel=\"Legit\",ax=ax_l, xticks=[],yticks=[],legend=False)\n",
        "  pd.DataFrame( list(zip(*hist_f)),columns=['bin', 'frequency']  ) \\\n",
        "    .set_index( 'bin' ) \\\n",
        "    .plot(kind='bar', xlabel=\"Fraud\",ax=ax_f, xticks=[],yticks=[],legend=False);\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL3buxKv0XE2"
      },
      "source": [
        "La differenza tra le due distribuzioni è meno evidente ad una analisi visiva, plottiamo quindi le prime features (che essendo risultato di una PCA sono responsabili della maggior parte della variazione) per essere sicuri della possibilità di separare le due classi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZyXWdAuQ9Sl"
      },
      "source": [
        "legit_x    =legit.select(\"V1\").toPandas()\n",
        "legit_y    =legit.select(\"V2\").toPandas()\n",
        "legit_z    =legit.select(\"V3\").toPandas()\n",
        "legit_time =legit.select(\"Time\").toPandas()\n",
        "\n",
        "fraud_x    =fraud.select(\"V1\").toPandas()\n",
        "fraud_y    =fraud.select(\"V2\").toPandas()\n",
        "fraud_z    =fraud.select(\"V3\").toPandas()\n",
        "fraud_time =fraud.select(\"Time\").toPandas()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(2,2,1,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V1,V2,V3\")\n",
        "ax.scatter(legit_x, legit_y, legit_z)\n",
        "ax.scatter(fraud_x, fraud_y, fraud_z)\n",
        "\n",
        "ax = fig.add_subplot(2,2,3)\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V1,V2\")\n",
        "ax.scatter(legit_x, legit_y)\n",
        "ax.scatter(fraud_x, fraud_y)\n",
        "\n",
        "ax = fig.add_subplot(2,2,2,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"Time,V1,V2\")\n",
        "ax.scatter(legit_time, legit_x, legit_y)\n",
        "ax.scatter(fraud_time, fraud_x, fraud_y)\n",
        "\n",
        "ax = fig.add_subplot(2,2,4)\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"Time,V1\")\n",
        "ax.scatter(legit_time, legit_x)\n",
        "ax.scatter(fraud_time, fraud_x)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJU7TUal40il"
      },
      "source": [
        "Vediamo come le transazioni fraudolente siano all'interno dello spazio delle features contigue e abbastanza compatte. Inoltre l'orario della transazione non sembra avere particolari correlazioni con la classe di appartenenza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8azg1HEm8vvb"
      },
      "source": [
        "## Classification\n",
        "\n",
        "Visto che le due classi sono fortemente sbilanciate, il classificatore verrà principalmente addestrato su campioni appartenenti alla classe delle transazioni legittime, introducendo un bias indesiderato. \n",
        "Per questo motivo utilizziamo delle tecniche per mitigare il problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N32y1EGmd2MG"
      },
      "source": [
        "# Definiamo una funzione per stampare l'area sotto la curva precision-recall e \n",
        "# la matrice di confusione per il training set.\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "\n",
        "def evaluate_model(cvModel, train, test, callable=True):\n",
        "  evaluator = cvModel.getEvaluator()\n",
        "  predictions = cvModel.transform(test)\n",
        "  performance = cvModel.transform(train)\n",
        "  summary = cvModel.bestModel.stages[2].summary() \\\n",
        "            if callable else cvModel.bestModel.stages[2].summary\n",
        "\n",
        "  auprc = evaluator.evaluate(performance)\n",
        "  print(f\"Area Under PR Curve (train): {(100*auprc):05.2f}%\")\n",
        "\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.plot([0, 1], [0, 1], 'r--')\n",
        "  plt.plot(summary.roc.select('FPR').collect(),\n",
        "          summary.roc.select('TPR').collect())\n",
        "  plt.xlabel('FPR')\n",
        "  plt.ylabel('TPR')\n",
        "  plt.title(\"Precision-Recall curve\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Test set results:\")\n",
        "  auprc = evaluator.evaluate(predictions)\n",
        "  print(f\"Area Under PR Curve:  {(100*auprc):05.2f}%\")\n",
        "\n",
        "  predictions.createOrReplaceTempView('predictions')\n",
        "  tp = spark.sql(\"\"\"SELECT COUNT(*) as c\n",
        "                    FROM predictions\n",
        "                    WHERE predictions.Class == 1 AND\n",
        "                    predictions.prediction == 1.0 \"\"\").collect()[0].c\n",
        "  print(f\"True positives:       {tp}\")\n",
        "  fp = spark.sql(\"\"\"SELECT COUNT(*) as c\n",
        "                    FROM predictions\n",
        "                    WHERE predictions.Class == 0 AND\n",
        "                    predictions.prediction == 1.0 \"\"\").collect()[0].c\n",
        "  print(f\"False positives:      {fp}\")\n",
        "  tn = spark.sql(\"\"\"SELECT COUNT(*) as c\n",
        "                    FROM predictions\n",
        "                    WHERE predictions.Class == 0 AND\n",
        "                    predictions.prediction == 0.0 \"\"\").collect()[0].c\n",
        "  print(f\"True negatives:       {tn}\")\n",
        "  fn = spark.sql(\"\"\"SELECT COUNT(*) as c\n",
        "                    FROM predictions\n",
        "                    WHERE predictions.Class == 1 AND\n",
        "                    predictions.prediction == 0.0 \"\"\").collect()[0].c\n",
        "  print(f\"False negatives:      {fn}\")\n",
        "\n",
        "  sn.heatmap([[tp, fp],[fn,tn]], annot=True, fmt='.3g', cbar=False)\\\n",
        "   .set_title(\"Confusion matrix\");\n",
        "\n",
        "  precision = (tp)/(tp+fp)\n",
        "  accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "  recall = (tp)/(tp+fn)\n",
        "  print(f\"Precision:            {precision:1.3f}\")\n",
        "  print(f\"Accuracy:             {accuracy:1.3f}\")\n",
        "  print(f\"Recall:               {recall:1.3f}\")\n",
        "  print(f\"F1:                   {(2*precision*recall)/(precision+recall):1.3f}\")\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxowFbAqJVw_"
      },
      "source": [
        "# L'evaluator sarà lo stesso per tutti i modelli\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Class\")\\\n",
        "                                           .setMetricName(\"areaUnderPR\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhHz5Z_eskNa"
      },
      "source": [
        "# Per ottenere train e test set tramite undersampling\n",
        "\n",
        "def undersample_split(fraud, legit):\n",
        "  train_f, test_f = fraud.randomSplit([0.8, 0.2])\n",
        "  train_l, test_l = legit.randomSplit([0.8, 0.2])\n",
        "  train_l_undersampled = train_l.sample(False, 1.0*fraud.count()/legit.count())\\\n",
        "      .limit (train_f.count())\n",
        "  \n",
        "  # The train test is balanced with 50/50 frauds and legit transactions\n",
        "  # to avoid bias towards legit transactions\n",
        "  train       = train_f.union(train_l_undersampled)\n",
        "  # The test set has the same distribution of the original dataset\n",
        "  test        = test_f.union(test_l)\n",
        "  return (train, test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCe_uWgbMKYt"
      },
      "source": [
        "# Necessary imports for all methods\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.feature import IndexToString"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dnfgLivyDDt"
      },
      "source": [
        "# L'indexer sarà lo stesso per tutti i modelli\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "train, test = undersample_split(fraud, legit)\n",
        "\n",
        "labelIndexer = StringIndexer(inputCol='Class',outputCol='Class_index').fit(train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRaXG6yTr5Nn"
      },
      "source": [
        "### Support vector machine con undersampling\n",
        "\n",
        "Come primo approccio proviamo ad utilizzare come classificatore una support vector machine lineare, inoltre sottocampioniamo le transazioni legittime per non avere un dataset sbilanciato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFMSsALhsgCB"
      },
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "train, test = undersample_split(fraud, legit)\n",
        "\n",
        "assembler = VectorAssembler().setInputCols(train.schema.names[0:-1])\\\n",
        "  .setOutputCol(\"features\")\n",
        "scaler = MinMaxScaler().setMin(0).setMax(1).setInputCol(\"features\") \\\n",
        "  .setOutputCol(\"scaled_features\")\n",
        "linearSVC = LinearSVC().setFeaturesCol(\"scaled_features\") \\\n",
        "  .setLabelCol(\"Class\")\n",
        "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel')\\\n",
        "  .setLabels(labelIndexer.labels)\n",
        "\n",
        "pipeline = Pipeline().setStages([assembler, \\\n",
        "                                scaler, \\\n",
        "                                linearSVC,\\\n",
        "                                labelConverter])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svIXvXeLuJdZ"
      },
      "source": [
        "# Costruiamo l'algoritmo di addestramento, come una 10-fold Cross-validation che si addestra su una \n",
        "# griglia di iperparametri:\n",
        "# -- l'indice di Gini, e l'entropia per controlla la purezza dei nodi foglia\n",
        "# -- il numero di bin cioè di categorie da generare per ogni feature categorica\n",
        "# -- la profondità massima dell'albero\n",
        "\n",
        "# Generiamo la griglia\n",
        "paramGrid = ParamGridBuilder().addGrid(linearSVC.regParam, [0.1, 0.2, 0.5])\\\n",
        "                              .addGrid(linearSVC.maxIter, [20,50,100,200])\\\n",
        "                              .build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdlvRBCQz6vH"
      },
      "source": [
        "cv = CrossValidator().setEstimator(pipeline)\\\n",
        "                     .setEvaluator(evaluator)\\\n",
        "                     .setEstimatorParamMaps(paramGrid)\\\n",
        "                     .setNumFolds(10)\\\n",
        "                     .setParallelism(2)\n",
        "\n",
        "cvModel_svm = cv.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYFTi3_Y0LXY"
      },
      "source": [
        "evaluate_model(cvModel_svm, train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5qShNZVpT9Q"
      },
      "source": [
        "### Random forest con undersampling\n",
        "\n",
        "Confrontiamo il risultato precedente con una random forest, addestrata con gli stessi dati."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxRBW-UStDA9"
      },
      "source": [
        "train, test = undersample_split(fraud, legit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z524nXDqHf0p"
      },
      "source": [
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier \n",
        "\n",
        "assembler = VectorAssembler().setInputCols(train.schema.names[0:-1])\\\n",
        "  .setOutputCol(\"features\")\n",
        "scaler = MinMaxScaler().setMin(0).setMax(1).setInputCol(\"features\") \\\n",
        "  .setOutputCol(\"scaled_features\")\n",
        "randomForest = RandomForestClassifier().setFeaturesCol(\"scaled_features\") \\\n",
        "  .setLabelCol(\"Class\")\n",
        "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel')\\\n",
        "  .setLabels(labelIndexer.labels)\n",
        "pipeline = Pipeline().setStages([assembler, \\\n",
        "                                scaler, \\\n",
        "                                randomForest,\\\n",
        "                                labelConverter])\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPOy1-fbJR3n"
      },
      "source": [
        "# Costruiamo l'algoritmo di addestramento, come una 10-fold Cross-validation che si addestra su una \n",
        "# griglia di iperparametri:\n",
        "# -- l'indice di Gini, e l'entropia per controlla la purezza dei nodi foglia\n",
        "# -- il numero di bin cioè di categorie da generare per ogni feature categorica\n",
        "# -- la profondità massima dell'albero\n",
        "\n",
        "# Generiamo la griglia\n",
        "paramGrid = ParamGridBuilder().addGrid(randomForest.maxBins,[25, 28, 31])\\\n",
        "                              .addGrid(randomForest.maxDepth,[4,6,8])\\\n",
        "                              .addGrid(randomForest.impurity,[\"entropy\",\"gini\"])\\\n",
        "                              .build()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwL1xD8pJw0r"
      },
      "source": [
        "cv = CrossValidator().setEstimator(pipeline)\\\n",
        "                     .setEvaluator(evaluator)\\\n",
        "                     .setEstimatorParamMaps(paramGrid)\\\n",
        "                     .setNumFolds(10)\n",
        "                     \n",
        "cvModel_random_forest = cv.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-cVVkGlJyS9"
      },
      "source": [
        "evaluate_model(cvModel_random_forest, train, test, callable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxwMJ8xqBy_4"
      },
      "source": [
        "La random forest ha performance migliori rispetto alla svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eEgZXYpGsen"
      },
      "source": [
        "### Selezione delle feature\n",
        "\n",
        "I modelli precedenti si comportano abbastanza bene, soprattutto la random forest. L'addestramento sta però avvenendo su un sottoinsieme molto piccolo dei dati sulle transazioni legittime, potremmo quindi non catturare delle caratteristiche importanti dei dati. Risulta quindi opportuno provare un oversampling. \n",
        "\n",
        "L'addestramento con una porzione così grande del dataset con vettori di 30 feature però è una operazione molto pesante, ed è anche possibile che l'elevata dimensionalità riduca il potere di discriminazione dei modelli (curse of dimensionality). Effettuiamo quindi delle operazioni di estrazione delle features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBZK-RwBNxVC"
      },
      "source": [
        "#### Sfruttamento dei modelli addestrati precedentemente\n",
        "\n",
        "Dai modelli appena addestrati è possibile ottenere informazioni riguardo l'importanza delle varie features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsjog__H09LG"
      },
      "source": [
        "##### **SVM**\n",
        "\n",
        "Visto che le feature provengono da una PCA sono linearmente non correlate tra loro, i pesi appresi dalla SVM possono quindi fornire informazione su quali feature sono più rilevanti per la separazione delle due classi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huvKc5GCO_Gw"
      },
      "source": [
        "cvModel_svm.bestModel.stages[2].coefficients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZInUVTWuqXzp"
      },
      "source": [
        "Scaliamo il valore assoluto dei coefficienti per avere somma unitaria e li riportiamo nella tabella seguente \n",
        "\n",
        "Feature | Weight\n",
        "-|-\n",
        "V21 | 0.1206\n",
        "V14 | 0.1009\n",
        "V2 | 0.0829\n",
        "V4 | 0.0755\n",
        "V11 | 0.0699\n",
        "V23 | 0.0489\n",
        "V10 | 0.0468\n",
        "V6 | 0.0409\n",
        "V12 | 0.0403\n",
        "V20 | 0.0397\n",
        "V27 | 0.0356\n",
        "V28 | 0.0315\n",
        "V3 | 0.0296\n",
        "V5 | 0.0284\n",
        "V22 | 0.0273\n",
        "V9 | 0.0233\n",
        "Amount | 0.0228\n",
        "V16 | 0.0221\n",
        "V7 | 0.0203\n",
        "V17 | 0.0200\n",
        "V8 | 0.0169\n",
        "V1 | 0.0144\n",
        "V25 | 0.0087\n",
        "V13 | 0.0081\n",
        "V18 | 0.0071\n",
        "V19 | 0.0068\n",
        "V24 | 0.0061\n",
        "V15 | 0.0043\n",
        "V26 | 0.0003\n",
        "Time | 0.0000\n",
        "\n",
        "\n",
        "Le prime sette features hanno più del 50% dell'importanza, e scartando le ultime 11 non si perde più del 10%.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmyeI1B-wPkw"
      },
      "source": [
        "legit_x    =legit.select(\"V21\").toPandas()\n",
        "legit_y    =legit.select(\"V14\").toPandas()\n",
        "legit_z    =legit.select(\"V2\").toPandas()\n",
        "\n",
        "fraud_x    =fraud.select(\"V21\").toPandas()\n",
        "fraud_y    =fraud.select(\"V14\").toPandas()\n",
        "fraud_z    =fraud.select(\"V2\").toPandas()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(2,2,1,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V21,V14,V2\")\n",
        "ax.scatter(legit_x, legit_y, legit_z)\n",
        "ax.scatter(fraud_x, fraud_y, fraud_z)\n",
        "\n",
        "ax = fig.add_subplot(2,2,2,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V14,V21,V2\")\n",
        "ax.scatter(legit_y, legit_x, legit_z)\n",
        "ax.scatter(fraud_y, fraud_x, fraud_z)\n",
        "\n",
        "ax = fig.add_subplot(2,2,3,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V21,V14,V2 (bottom view)\")\n",
        "ax.scatter(legit_x, legit_y, legit_z)\n",
        "ax.scatter(fraud_x, fraud_y, -fraud_z)\n",
        "\n",
        "ax = fig.add_subplot(2,2,4,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V14,V21,V2 (bottom view)\")\n",
        "ax.scatter(legit_y, legit_x, legit_z)\n",
        "ax.scatter(fraud_y, fraud_x, -fraud_z)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am4th4U_ygMr"
      },
      "source": [
        "Osserviamo che considerando le sole tre features con i pesi maggiori è già possibile distinguere abbastanza nitidamente le due classi, meglio di quanto visto usando come features V1, V2, V3, e il Tempo, che effettivamente risultano essere abbastanza in basso nella lista ed eccezione di V2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTo0LxmRIzOH"
      },
      "source": [
        "###### _Performance su un numero di feature ridotto:_\n",
        "\n",
        "Proviamo ad addestrare la SVM utilizzando solamente le 7 feature associate a pesi maggiori."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym10aS4UIjKP"
      },
      "source": [
        "train, test = undersample_split(fraud, legit)\n",
        "\n",
        "\n",
        "assembler = VectorAssembler().setInputCols([\"V21\", \"V14\", \"V2\", \"V4\",\n",
        "                                            \"V11\", \"V23\", \"V10\" ])\\\n",
        "  .setOutputCol(\"features\")\n",
        "scaler = MinMaxScaler().setMin(0).setMax(1).setInputCol(\"features\") \\\n",
        "  .setOutputCol(\"scaled_features\")\n",
        "linearSVC = LinearSVC().setFeaturesCol(\"scaled_features\") \\\n",
        "  .setLabelCol(\"Class\")\n",
        "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel')\\\n",
        "  .setLabels(labelIndexer.labels)\n",
        "\n",
        "pipeline = Pipeline().setStages([assembler, \\\n",
        "                                scaler, \\\n",
        "                                linearSVC,\\\n",
        "                                labelConverter])\n",
        "\n",
        "# Generiamo la griglia\n",
        "paramGrid = ParamGridBuilder().addGrid(linearSVC.regParam, [0.1, 0.2, 0.5])\\\n",
        "                              .addGrid(linearSVC.maxIter, [20,50,100,200])\\\n",
        "                              .build()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQSKFD0tKX9b"
      },
      "source": [
        "cv = CrossValidator().setEstimator(pipeline)\\\n",
        "                     .setEvaluator(evaluator)\\\n",
        "                     .setEstimatorParamMaps(paramGrid)\\\n",
        "                     .setNumFolds(10)\n",
        "\n",
        "cvModel_svm_reduced = cv.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_fbq_7KKRpU"
      },
      "source": [
        "evaluate_model(cvModel_svm_reduced, train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEjL6SVliaYK"
      },
      "source": [
        "La performance del classificatore è leggermente aumentata rimuovendo le feature meno importanti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "058u01Pi04Wh"
      },
      "source": [
        "##### **Random Forest**\n",
        "\n",
        "Ripetiamo un procedimento simile sulla random forest, che precedentemente dava migliori risultati nella classificazione."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfjAnUQ1OJsr"
      },
      "source": [
        "cvModel_random_forest.bestModel.stages[2].featureImportances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMdCaZxKPK-f"
      },
      "source": [
        "Per comodità di visione si riportano i dati in output al comando precedente anche in formato tabellare.\n",
        "\n",
        "Feature | Importance\n",
        "-|-\n",
        " V14 |  0.191\n",
        " V17 |  0.1691\n",
        " V10 |  0.14\n",
        " V12 |  0.1082\n",
        " V4 |  0.1041\n",
        " V11 |  0.0341\n",
        " V7 |  0.0309\n",
        " V16 |  0.0267\n",
        " V3 |  0.0166\n",
        " Amount |  0.0159\n",
        " V20 |  0.0134\n",
        " V19 |  0.0131\n",
        " V21 |  0.0128\n",
        " V8 |  0.0126\n",
        " V5 |  0.0121\n",
        " V2 |  0.0106\n",
        " V9 |  0.0096\n",
        " V13 |  0.0084\n",
        " V22 |  0.0076\n",
        " V24 |  0.0074\n",
        " V27 |  0.0071\n",
        " V18 |  0.0069\n",
        " V1 |  0.0069\n",
        " V15 |  0.0065\n",
        " Time |  0.0059\n",
        " V25 |  0.0057\n",
        " V6 |  0.0052\n",
        " V23 |  0.0052\n",
        " V28 |  0.0038\n",
        " V26 |  0.0025\n",
        "\n",
        "Le prime tre features da sole hanno il 50% dell'importanza, e per arrivare al 90% se ne possono scartare metà."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTz43zHRQyX7"
      },
      "source": [
        "legit_x    =legit.select(\"V14\").toPandas()\n",
        "legit_y    =legit.select(\"V17\").toPandas()\n",
        "legit_z    =legit.select(\"V10\").toPandas()\n",
        "\n",
        "fraud_x    =fraud.select(\"V14\").toPandas()\n",
        "fraud_y    =fraud.select(\"V17\").toPandas()\n",
        "fraud_z    =fraud.select(\"V10\").toPandas()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(2,2,1,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V14,V17,V10\")\n",
        "ax.scatter(legit_x, legit_y, legit_z)\n",
        "ax.scatter(fraud_x, fraud_y, fraud_z)\n",
        "\n",
        "ax = fig.add_subplot(2,2,2,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V17,V14,V10\")\n",
        "ax.scatter(legit_y, legit_x, legit_z)\n",
        "ax.scatter(fraud_y, fraud_x, fraud_z)\n",
        "\n",
        "ax = fig.add_subplot(2,2,3,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V14,V17,V10 (bottom view)\")\n",
        "ax.scatter(legit_x, legit_y, legit_z)\n",
        "ax.scatter(fraud_x, fraud_y, -fraud_z)\n",
        "\n",
        "ax = fig.add_subplot(2,2,4,projection='3d')\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.set_zticklabels([])\n",
        "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
        "ax.set_title(\"V17,V14,V10 (bottom view)\")\n",
        "ax.scatter(legit_y, legit_x, legit_z)\n",
        "ax.scatter(fraud_y, fraud_x, -fraud_z)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzWS8orVVEwO"
      },
      "source": [
        "Ad una ispezione visiva sembra evidente come, utilizzando le prime tre feature, le due classi risultino meglio separate di quanto non succedesse per la SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D43UxP6PvxQ"
      },
      "source": [
        "###### _Performance su numero di feature ridotto_:\n",
        "Proviamo ad addestrare la random forest utilizzando soltanto le prime tre feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEkwiXXUP1GI"
      },
      "source": [
        "assembler = VectorAssembler().setInputCols([\"V14\",\"V17\",\"V10\" ])\\\n",
        "  .setOutputCol(\"features\")\n",
        "scaler = MinMaxScaler().setMin(0).setMax(1).setInputCol(\"features\") \\\n",
        "  .setOutputCol(\"scaled_features\")\n",
        "randomForest = RandomForestClassifier().setFeaturesCol(\"scaled_features\") \\\n",
        "  .setLabelCol(\"Class\")\n",
        "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel')\\\n",
        "  .setLabels(labelIndexer.labels)\n",
        "pipeline = Pipeline().setStages([assembler, \\\n",
        "                                scaler, \\\n",
        "                                randomForest,\\\n",
        "                                labelConverter])\n",
        "\n",
        "paramGrid = ParamGridBuilder().addGrid(randomForest.maxBins,[25, 28, 31])\\\n",
        "                              .addGrid(randomForest.maxDepth,[4,6,8])\\\n",
        "                              .addGrid(randomForest.impurity,[\"entropy\",\"gini\"])\\\n",
        "                              .build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikRc1X1pP1GI"
      },
      "source": [
        "cv = CrossValidator().setEstimator(pipeline)\\\n",
        "                     .setEvaluator(evaluator)\\\n",
        "                     .setEstimatorParamMaps(paramGrid)\\\n",
        "                     .setNumFolds(10)\n",
        "                     \n",
        "cvModel_random_forest_reduced = cv.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScBxgrwFP1GI"
      },
      "source": [
        "evaluate_model(cvModel_random_forest_reduced, train, test, callable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0-oqP_PP1GJ"
      },
      "source": [
        "Vediamo come usare soltanto le prime tre feature permette di ottenere risultati estremamente vicini a quelli ottenuti usando tutte e trenta le feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzTX_CRPIj_E"
      },
      "source": [
        "### Addestramento dei modelli precedenti su un sottoinsieme di feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pxmtt1-2jVA"
      },
      "source": [
        "Al seguito delle osservazioni precedenti prendiamo in considerazione le feature `[\"V21\", \"V14\", \"V2\", \"V4\", \"V11\", \"V23\", \"V10\", \"V17\" ]`, frutto dell'unione dei due insiemi di feature identificati precedentemente.\n",
        "\n",
        "Proviamo ad estrarre una loro combinazione non lineare con più potere rappresentativo attraverso un autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuZ-b1O2t6W2"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "\n",
        "\n",
        "train, test = undersample_split(fraud, legit)\n",
        "\n",
        "assembler = VectorAssembler().setInputCols([\"V21\", \"V14\",  \"V2\", \"V4\",\n",
        "                                            \"V11\", \"V23\", \"V10\", \"V17\" ])\\\n",
        "  .setOutputCol(\"features\")\n",
        "scaler = MinMaxScaler().setMin(0).setMax(1).setInputCol(\"features\") \\\n",
        "  .setOutputCol(\"scaled_features\")\n",
        "mlp = MultilayerPerceptronClassifier(layers=[8,5,1]).setFeaturesCol(\"scaled_features\") \\\n",
        "  .setLabelCol(\"Class\")\n",
        "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel')\\\n",
        "  .setLabels(labelIndexer.labels)\n",
        "\n",
        "pipeline = Pipeline().setStages([assembler, \\\n",
        "                                scaler, \\\n",
        "                                mlp,\\\n",
        "                                labelConverter])\n",
        "\n",
        "# Generiamo la griglia\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "                              .addGrid(mlp.maxIter, [20,50,100,200])\\\n",
        "                              .build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOWY8kSIv-YK"
      },
      "source": [
        "cv = CrossValidator().setEstimator(pipeline)\\\n",
        "                     .setEvaluator(evaluator)\\\n",
        "                     .setEstimatorParamMaps(paramGrid)\\\n",
        "                     .setNumFolds(10)\n",
        "\n",
        "cvModel_mlp = pipeline.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8WmOicdwWTc"
      },
      "source": [
        "evaluate_model(cvModel_mlp, train, test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}